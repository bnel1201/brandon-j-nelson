[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Staff Fellow - FDA/CDRH/OSEL/DIDSR\nRegulatory Scientist in FDA’s Division of Imaging, Diagnostics, and Software Reliability (DIDSR)\nI conduct regulatory science research, the science of evaluating medical devices, specifically in improving the evaluation of the fast moving field of radiology devices.\nPreviously completed a PhD at Mayo Clinic’s CT Clinical Innovation Center: Thesis on preclinical applications of X-ray phase contrast CT\nDisclaimer: The views expressed on this site are my own and do not reflect the views of my employer\nORCID: 0000-0001-9213-3131\nGoogle Scholar\nResearchGate\nCV"
  },
  {
    "objectID": "index.html#staff-fellow-regulatory-scientist-fdacdrhoseldidsr",
    "href": "index.html#staff-fellow-regulatory-scientist-fdacdrhoseldidsr",
    "title": "Brandon J. Nelson",
    "section": "Staff Fellow | Regulatory Scientist @ FDA/CDRH/OSEL/DIDSR",
    "text": "Staff Fellow | Regulatory Scientist @ FDA/CDRH/OSEL/DIDSR"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Brandon J. Nelson",
    "section": "About Me",
    "text": "About Me\nI joined FDA’s Division of Imaging, Diagnostics and Software Reliability (DIDSR) in July, 2022 as an ORISE Fellow investigating generalizability of deep learning image reconstruction for Computed Tomography (CT), and have been a Staff Fellow since Jan, 2023. My research interests lay at the intersection of artificial intelligence and machine learning (AI/ML) and pediatric radiology. It goes without saying that pediatric patients are not just small adults, their anatomy and disease presentation varies dramatically from birth to adulthood. Pediatric patients have the most to gain from advancements in radiology given their longer life time horizons, yet they are a vulnerable population and are often inadequately represented in medical device development and evaluation. The goal of my research projects are to improve device evaluation of pediatric patients by better incorporating the unique aspects of pediatric anatomy, disease, and imaging protocols. These projects rely upon a range of technologies ranging from digital human phantoms, imaging device simulations, and deep learning."
  },
  {
    "objectID": "index.html#opportunities",
    "href": "index.html#opportunities",
    "title": "Brandon J. Nelson",
    "section": "Opportunities",
    "text": "Opportunities\nI am currently looking for a talented and motivated postdoctoral fellow (or similar trainee with relevant experience) to partner with me on an upcoming project investigating and developing tools to improve the evaluation of intracranial hemorrhage computer aided triage devices (CADt) in pediatric and other underrepresented patient groups.\n\nTo learn more, please reach out to me with your CV and availability at brandon.nelson at fda.hhs.gov"
  },
  {
    "objectID": "index.html#posts",
    "href": "index.html#posts",
    "title": "Brandon J. Nelson",
    "section": "Posts",
    "text": "Posts"
  },
  {
    "objectID": "posts/2021-01-29-vandermonde matrices in beam hardening corrections.html",
    "href": "posts/2021-01-29-vandermonde matrices in beam hardening corrections.html",
    "title": "Application of Vandermonde Interpolation in CT beam hardening correction",
    "section": "",
    "text": "this notebook is available as a colab notebook\n\nNow that we’ve seen how a nonlinear 1D signal can be made linear by using Vandermonde interpolation, we can quickly expand our discussion to images by treating each pixel as a 1D function of intensity to be corrected.\nLet’s first see how a nonlinearity introduced in our project data is reflected in the reconstructed image…\nWe start with a simplified model of beam hardening described in: https://aapm.onlinelibrary.wiley.com/doi/full/10.1118/1.2742501\nwhere the polychromatic attenuation coefficient \\(\\langle \\mu \\rangle\\) can be modeled in terms of the true attenuation coefficient \\(\\mu\\) and penetration distance \\(x\\)\n\\[\\begin{equation}\n\\langle \\mu \\rangle = \\frac{\\mu_0}{1+\\lambda x}\n\\end{equation}\\]\nNote: \\(\\lambda\\) is a fitting parameter (not wavelength) that controls the strength of beam hardening. If \\(\\lambda = 0\\) then there is no beam hardening. If $= $ then \\(\\langle \\mu \\rangle = 0\\)\nLimitation: Because we cannot easily separate \\(x\\) and \\(\\mu\\) we know that in the ideal case \\(-ln(I/I_0) = \\int \\mu dx\\) thus here we need to make the assumption that materials have about equal \\(\\mu\\) (i.e.) that \\(\\mu\\) doesn’t contribute to beam hardening (which we know not to be true because bone will harden more than water). But we’ll make this assumption just for this demonstration.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nx = np.linspace(0, 1)\nlam = 0.1 #set small amount of beam hardening\nmu = x\ndef beamHardening(x, lam=1): #here we cannot separate mu and penetration distance so we have to assume them to be equal\n  return x/(1+lam*x)\n\nplt.plot(x, mu, label='ideal')\nplt.plot(x, beamHardening(x, lam), label = 'real')\nplt.xlabel('Penetration depth [cm]')\nplt.ylabel('Measured value')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.plot(mu, mu, label='monochromatic')\nplt.plot(mu, beamHardening(mu), label ='polychromatic')\nplt.xlabel('ideal attenuation coefficient [cm^-1]')\nplt.ylabel('meaudred attenuation coefficient [cm^-1]')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom skimage.data import shepp_logan_phantom\nfrom skimage.transform import radon, rescale, iradon\n\np = shepp_logan_phantom()\np = rescale(p, scale=1, mode='reflect', multichannel=False)\ntheta = np.linspace(0., 360., 360, endpoint=False)\nr = radon(p, theta=theta)\nideal = iradon(r, theta=theta)\ncorrupted_r = beamHardening(r, lam=0.1) #here we cannot separate mu and penetration distance so we have to assume them to be equal\nuncorrected = iradon(corrupted_r, theta=theta)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6)) \n\nax1.set_title(\"Original\")\nax1.imshow(ideal, cmap=plt.cm.Greys_r)\n\nax2.set_title(\"Simulated Beam Hardening\")\nax2.imshow(uncorrected, cmap=plt.cm.Greys_r)\n          \nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nidealLine=ideal[:,int(ideal.shape[0]/2)]\nuncorrectedLine=uncorrected[:,int(ideal.shape[0]/2)]\nuncorrectedLine/=max(uncorrectedLine) #normalized for easier comparison\nplt.plot(idealLine)\nplt.plot(uncorrectedLine)\nplt.legend([\"ideal\",\"uncorrected\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\nUsed by Chabior 2011\n\ncorrectionLineFunc = idealLine/uncorrectedLine\nplt.plot(correctionLineFunc)\n\n\n\n\n\n\n\n\nThis is clearly\nLet’s assume a 0th order correction, a polynomial with degree 1, \\(N=1\\) \\(p = \\sum_{n} cq^{n} = c_0 + c_1 q\\), but we assume \\(c_0 = 0\\) so \\(p=cq\\)\n\nplt.plot(idealLine)\nplt.plot(uncorrectedLine)\nplt.plot(uncorrectedLine*correctionLineFunc)\nplt.legend([\"ideal\",\"uncorrected\",\"\"])\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(correctionLineFunc.shape)\nprint(corrupted_r.shape)\npc = np.zeros([corrupted_r.shape[0], corrupted_r.shape[1]])\nfor i in np.arange(1,corrupted_r.shape[1]):\n  pc[:,i] = corrupted_r[:,i]*correctionLineFunc\n\n(400,)\n(400, 360)\n\n\n\nchabior_corrected = iradon(pc, theta=theta)\n\nplt.imshow(chabior_corrected, cmap = 'gray')\nplt.title(\"Chabior method applied to CT\")\nplt.show()\n\n\n\n\n\n\n\n\nIn our first imaging example we will start by fitting one polynomial fit function to applied to uniformly to every pixel in our 400x400 pixel image.\nWe can generate our new Vandermonde matrix, let’s start with \\(n=3\\) monomial terms, since each image has \\(400^2=160000\\) pixels our matrix will look like\n\\[\\begin{bmatrix}\n1       & x_0   & x_0^2      \\\\\n1       & x_1   & x_1^2      \\\\\n\\vdots  &\\vdots &\\vdots      \\\\\n1       & x_m   &x_{15999}^2 \\\\\n\\end{bmatrix}\\]\n\ndef poly_recon(A, theta):\n  \"\"\"Argument is projection vandermonde matrix [proj.ravel() X view angles] and theta array, returns recon.ravel() X deg matrix of vandermonde recons\"\"\"\n  deg=A.shape[1]\n  sz = int(A.shape[0]/theta.size)\n  A=A.reshape([sz,theta.size,deg])\n  f = np.zeros([sz*sz,deg])\n  for i in range(deg):\n      fi=iradon(A[:,:,i],theta=theta, circle=True)\n      f[:,i]=fi.ravel()\n  return f\n  \ndef vanderRecon(proj, theta, deg):\n  \"\"\"Argument is [xpix*ypix X theta] projection, returns [recon.ravel() X deg] matrix of vandermonde reconstructed projections\"\"\"\n  A=np.vander(corrupted_r.ravel(),deg, increasing=True)\n  return poly_recon(A,theta);\n\nLet’s confirm that the size of the Vandermonde matrix of our recons is what we expect…\n\ndeg=3 #set number of monomial terms\nA = vanderRecon(corrupted_r, theta, deg)\nprint(A.shape)\n\n(160000, 3)\n\n\n\nA_rank = np.linalg.matrix_rank(A)\nprint(A_rank)\n\n3\n\n\nNow let’s define our correction function once we determine our coefficients and a loss function to define our mean squared error MSE\n\ndef mse(y_hat,y):\n   return ((y_hat-y)**2).mean()\n\n\ndeg=8 #set number of monomial terms\nA = vanderRecon(corrupted_r, theta, deg)\nc = np.linalg.lstsq(A,ideal.ravel(),rcond=None)[0]\ncorrected_proj=np.polynomial.polynomial.polyval(corrupted_r, c).reshape(corrupted_r.shape)\n\nuncorrected = iradon(corrupted_r, theta=theta, circle=True)\ncorrected   = iradon(corrected_proj, theta=theta, circle=True)\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 6))\n\nax1.set_title(\"Original\")\nax1.imshow(ideal, cmap=plt.cm.Greys_r)\n\nax2.set_title(\"Simulated Beam Hardening \\n MSE = \" + str(mse(uncorrected,ideal)))\nax2.imshow(uncorrected, cmap=plt.cm.Greys_r)\n\nax3.set_title(\"Linearized \\n MSE = \" + str(mse(corrected,ideal)))\nax3.imshow(corrected, cmap=plt.cm.Greys_r)\n          \nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ncorrectedLine=corrected[:,int(ideal.shape[0]/2)]\nplt.plot(idealLine)\nplt.plot(uncorrectedLine)\nplt.plot(correctedLine)\nplt.legend([\"ideal\",\"uncorrected\",\"corrected\"])\nplt.show()\n\n\n\n\n\n\n\n\n#Now let’s add a spatially varying component#\n\nimport random\ndef add_lines(r):\n    n_lines = int(random.random()*40)\n    for i in range(n_lines):\n      line_intensity = random.uniform(0.5,1)\n      line_width = int(10*random.random())\n      line_location = int(random.random()*r.shape[0])\n      r[line_location:line_location+line_width,:]*=line_intensity\n    return r\n\n\ndef corruptProjection(proj, bh_strength = 0.01, spatialDependence_strength = 1):\n  m = np.ones(proj.shape)*(1-spatialDependence_strength) + add_lines(np.ones(proj.shape))*spatialDependence_strength\n  corrupted_r = beamHardening(proj*m, lam = bh_strength)\n  return m, corrupted_r \n\n\nm, corrupted_r=corruptProjection(r, bh_strength=0.1)\nplt.imshow(corrupted_r,cmap=plt.cm.Greys_r)\nplt.show()\nplt.plot(corrupted_r[:,100])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nuncorrected = iradon(corrupted_r, theta=theta, circle=True)\nplt.imshow(uncorrected,cmap=plt.cm.Greys_r)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nhttps://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.polynomial.polynomial.polyvander2d.html\n\ndef vanderComborecon(q,m,theta,deg):\n  qm = np.polynomial.polynomial.polyvander2d(q.ravel(), m.ravel(), [deg-1, deg-1])\n  return poly_recon(qm,theta)\n\n\ndeg = 6\nf = vanderComborecon(corrupted_r,m,theta, deg)\nprint(\"The Vandermonde matrix has a shape of [{0:d}, {1:d}] and a condition number of: {2:.0f}\".format(f.shape[0], f.shape[1], np.linalg.cond(f)))\n\nThe Vandermonde matrix has a shape of [160000, 36] and a condition number of: 3367960979906\n\n\nNote that the condition number of the \\(qm\\) matrix is very large, meaning that it is pretty poorly conditioned, thus our algorithm will be very sensitive to different input values. Is there a way to decrease the condition number and better condition our problem?\nCondition number is a measure of the sensitivty of a problem to error in input. A large condition number number indicates an unstable result that could vary drastically depending on minor errors in the input.\nA large condition number is a known problem of vandermonde matrix inversions.\nhttps://en.wikipedia.org/wiki/Condition_number\n\nc=np.linalg.lstsq(f,ideal.ravel(),rcond=None)[0]\nplt.plot(c)\nplt.show() \n\n\n\n\n\n\n\n\nLooking at our coefficients above and how large and widely varying they are is a result of the problem being poorly conditioned, not only do these coefficients vary widely with small changes in input (try re-running the program again with noise added), but also the coefficients aren’t very intuitive at all.\n\ndef square(c):\n  return c.reshape([int(np.sqrt(len(c))), -1])\n\ndef performCorrection2D(corrupted_r, m, c):\n  return np.polynomial.polynomial.polyval2d(corrupted_r.ravel(), m.ravel(), square(c)).reshape(corrupted_r.shape)\n\n\n\n\nRecall the correction is \\(\\hat{p} = c_{i,j} q^i M^j\\) thus the coordinates in the image below indicate the monomial and its relative contribution to the polynomial\n\ndef correctAndDisplay(corrupted_r, m, c, theta):\n  p = performCorrection2D(corrupted_r, m, c)\n  corrected = iradon(p, theta=theta, circle=True)\n  plt.imshow(corrected, cmap=plt.cm.Greys_r)\n  ms_error = mse(corrected,ideal)\n  plt.title(\"Corrected \\n MSE = \" + str(ms_error))\n  return ms_error\n\ncorrectAndDisplay(corrupted_r, m, c, theta)\n\n0.00014014461661890804\n\n\n\n\n\n\n\n\n\n\ndef visualizeCoefficients(c):\n  plt.imshow(square(c))\n  plt.xlabel('M')\n  plt.ylabel('q')\n  plt.colorbar()\n  plt.title('Monomial contributions')\n\nvisualizeCoefficients(c)\n\n\n\n\n\n\n\n\n\np = performCorrection2D(corrupted_r, m, c)\ncorrected = iradon(p, theta=theta, circle=True)\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 12))\n\nax1.set_title(\"Original\")\nax1.imshow(ideal, cmap=plt.cm.Greys_r)\n\nax2.set_title(\"Corrupted \\n MSE = \" + str(mse(uncorrected,ideal)))\nax2.imshow(uncorrected, cmap=plt.cm.Greys_r)\n\nax3.set_title(\"Corrected \\n MSE = \" + str(mse(corrected,ideal)))\nax3.imshow(corrected, cmap=plt.cm.Greys_r)\n          \nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNot bad results for 10% beam hardening!\n\n\n\n\ndef find2Dpolynomialcoefficients(corrupted_r, m, theta, deg):\n  f = vanderComborecon(corrupted_r,m,theta, deg)\n  return np.linalg.lstsq(f,ideal.ravel(),rcond=None)[0]\n\ndef plotAndVisualizeDegreeDependence(corrupted_r, m, theta, maxDegree=10):\n  ms_error = []\n  for d in np.arange(2, maxDegree):\n    coefficients = find2Dpolynomialcoefficients(corrupted_r, m, theta, d)\n\n    plt.figure(figsize=[14,4])\n    plt.subplot(1,3,1)\n    visualizeCoefficients(coefficients)\n    plt.subplot(1,3,2)\n    err = correctAndDisplay(corrupted_r, m, coefficients, theta)\n    ms_error.append(err)\n    plt.subplot(1,3,3)\n    plt.plot(np.arange(2, d+1), ms_error)\n    plt.xlabel('Polynomial degree')\n    plt.ylabel('MSE')\n    plt.show()\n\nplotAndVisualizeDegreeDependence(corrupted_r, m, theta)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s clear that MSE decreases and asymptotes as the polynomial degrees increase (but at the cost of\n\n\n\nTry this again when there is no spatial component to the beam hardening, how does this affect the M dependency?\n\nnoSpatialm, noSpatial_corrupted_r = corruptProjection(r, bh_strength=0.1, spatialDependence_strength = 0)\nplotAndVisualizeDegreeDependence(noSpatial_corrupted_r, noSpatialm, theta)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally repeat once more with Beam hardening strength set to zero and spatiall varying strength on max, how does this affect the coefficient activation map?\n\nnoBHm, noBH_corrupted_r = corruptProjection(r, bh_strength=0, spatialDependence_strength = 1)\nplotAndVisualizeDegreeDependence(noBH_corrupted_r, noBHm, theta)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObserve how the MSE decreases then increases at higher degrees. This demonstrates a weakness of Vandermonde polynomial interpolation that as the Vandermonde matrix becomes larger its condition number grows dramatically and the result becomes unstable.\nMove on to the last notebook on iterative methods for solving the Vandermonde interpolation least squares problem: https://colab.research.google.com/drive/1kErNKAbJlJUpq2jsb0roMJA7kTNB9vY0"
  },
  {
    "objectID": "posts/2021-01-29-vandermonde matrices in beam hardening corrections.html#comparison-to-kak-and-slaney-ch.-4-method",
    "href": "posts/2021-01-29-vandermonde matrices in beam hardening corrections.html#comparison-to-kak-and-slaney-ch.-4-method",
    "title": "Application of Vandermonde Interpolation in CT beam hardening correction",
    "section": "",
    "text": "Used by Chabior 2011\n\ncorrectionLineFunc = idealLine/uncorrectedLine\nplt.plot(correctionLineFunc)\n\n\n\n\n\n\n\n\nThis is clearly\nLet’s assume a 0th order correction, a polynomial with degree 1, \\(N=1\\) \\(p = \\sum_{n} cq^{n} = c_0 + c_1 q\\), but we assume \\(c_0 = 0\\) so \\(p=cq\\)\n\nplt.plot(idealLine)\nplt.plot(uncorrectedLine)\nplt.plot(uncorrectedLine*correctionLineFunc)\nplt.legend([\"ideal\",\"uncorrected\",\"\"])\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(correctionLineFunc.shape)\nprint(corrupted_r.shape)\npc = np.zeros([corrupted_r.shape[0], corrupted_r.shape[1]])\nfor i in np.arange(1,corrupted_r.shape[1]):\n  pc[:,i] = corrupted_r[:,i]*correctionLineFunc\n\n(400,)\n(400, 360)\n\n\n\nchabior_corrected = iradon(pc, theta=theta)\n\nplt.imshow(chabior_corrected, cmap = 'gray')\nplt.title(\"Chabior method applied to CT\")\nplt.show()\n\n\n\n\n\n\n\n\nIn our first imaging example we will start by fitting one polynomial fit function to applied to uniformly to every pixel in our 400x400 pixel image.\nWe can generate our new Vandermonde matrix, let’s start with \\(n=3\\) monomial terms, since each image has \\(400^2=160000\\) pixels our matrix will look like\n\\[\\begin{bmatrix}\n1       & x_0   & x_0^2      \\\\\n1       & x_1   & x_1^2      \\\\\n\\vdots  &\\vdots &\\vdots      \\\\\n1       & x_m   &x_{15999}^2 \\\\\n\\end{bmatrix}\\]\n\ndef poly_recon(A, theta):\n  \"\"\"Argument is projection vandermonde matrix [proj.ravel() X view angles] and theta array, returns recon.ravel() X deg matrix of vandermonde recons\"\"\"\n  deg=A.shape[1]\n  sz = int(A.shape[0]/theta.size)\n  A=A.reshape([sz,theta.size,deg])\n  f = np.zeros([sz*sz,deg])\n  for i in range(deg):\n      fi=iradon(A[:,:,i],theta=theta, circle=True)\n      f[:,i]=fi.ravel()\n  return f\n  \ndef vanderRecon(proj, theta, deg):\n  \"\"\"Argument is [xpix*ypix X theta] projection, returns [recon.ravel() X deg] matrix of vandermonde reconstructed projections\"\"\"\n  A=np.vander(corrupted_r.ravel(),deg, increasing=True)\n  return poly_recon(A,theta);\n\nLet’s confirm that the size of the Vandermonde matrix of our recons is what we expect…\n\ndeg=3 #set number of monomial terms\nA = vanderRecon(corrupted_r, theta, deg)\nprint(A.shape)\n\n(160000, 3)\n\n\n\nA_rank = np.linalg.matrix_rank(A)\nprint(A_rank)\n\n3\n\n\nNow let’s define our correction function once we determine our coefficients and a loss function to define our mean squared error MSE\n\ndef mse(y_hat,y):\n   return ((y_hat-y)**2).mean()\n\n\ndeg=8 #set number of monomial terms\nA = vanderRecon(corrupted_r, theta, deg)\nc = np.linalg.lstsq(A,ideal.ravel(),rcond=None)[0]\ncorrected_proj=np.polynomial.polynomial.polyval(corrupted_r, c).reshape(corrupted_r.shape)\n\nuncorrected = iradon(corrupted_r, theta=theta, circle=True)\ncorrected   = iradon(corrected_proj, theta=theta, circle=True)\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 6))\n\nax1.set_title(\"Original\")\nax1.imshow(ideal, cmap=plt.cm.Greys_r)\n\nax2.set_title(\"Simulated Beam Hardening \\n MSE = \" + str(mse(uncorrected,ideal)))\nax2.imshow(uncorrected, cmap=plt.cm.Greys_r)\n\nax3.set_title(\"Linearized \\n MSE = \" + str(mse(corrected,ideal)))\nax3.imshow(corrected, cmap=plt.cm.Greys_r)\n          \nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ncorrectedLine=corrected[:,int(ideal.shape[0]/2)]\nplt.plot(idealLine)\nplt.plot(uncorrectedLine)\nplt.plot(correctedLine)\nplt.legend([\"ideal\",\"uncorrected\",\"corrected\"])\nplt.show()\n\n\n\n\n\n\n\n\n#Now let’s add a spatially varying component#\n\nimport random\ndef add_lines(r):\n    n_lines = int(random.random()*40)\n    for i in range(n_lines):\n      line_intensity = random.uniform(0.5,1)\n      line_width = int(10*random.random())\n      line_location = int(random.random()*r.shape[0])\n      r[line_location:line_location+line_width,:]*=line_intensity\n    return r\n\n\ndef corruptProjection(proj, bh_strength = 0.01, spatialDependence_strength = 1):\n  m = np.ones(proj.shape)*(1-spatialDependence_strength) + add_lines(np.ones(proj.shape))*spatialDependence_strength\n  corrupted_r = beamHardening(proj*m, lam = bh_strength)\n  return m, corrupted_r \n\n\nm, corrupted_r=corruptProjection(r, bh_strength=0.1)\nplt.imshow(corrupted_r,cmap=plt.cm.Greys_r)\nplt.show()\nplt.plot(corrupted_r[:,100])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nuncorrected = iradon(corrupted_r, theta=theta, circle=True)\nplt.imshow(uncorrected,cmap=plt.cm.Greys_r)\nplt.show()"
  },
  {
    "objectID": "posts/2021-01-29-vandermonde matrices in beam hardening corrections.html#numpy-has-a-built-in-2d-vandermonde-matrix-that-we-can-use-here",
    "href": "posts/2021-01-29-vandermonde matrices in beam hardening corrections.html#numpy-has-a-built-in-2d-vandermonde-matrix-that-we-can-use-here",
    "title": "Application of Vandermonde Interpolation in CT beam hardening correction",
    "section": "",
    "text": "https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.polynomial.polynomial.polyvander2d.html\n\ndef vanderComborecon(q,m,theta,deg):\n  qm = np.polynomial.polynomial.polyvander2d(q.ravel(), m.ravel(), [deg-1, deg-1])\n  return poly_recon(qm,theta)\n\n\ndeg = 6\nf = vanderComborecon(corrupted_r,m,theta, deg)\nprint(\"The Vandermonde matrix has a shape of [{0:d}, {1:d}] and a condition number of: {2:.0f}\".format(f.shape[0], f.shape[1], np.linalg.cond(f)))\n\nThe Vandermonde matrix has a shape of [160000, 36] and a condition number of: 3367960979906\n\n\nNote that the condition number of the \\(qm\\) matrix is very large, meaning that it is pretty poorly conditioned, thus our algorithm will be very sensitive to different input values. Is there a way to decrease the condition number and better condition our problem?\nCondition number is a measure of the sensitivty of a problem to error in input. A large condition number number indicates an unstable result that could vary drastically depending on minor errors in the input.\nA large condition number is a known problem of vandermonde matrix inversions.\nhttps://en.wikipedia.org/wiki/Condition_number\n\nc=np.linalg.lstsq(f,ideal.ravel(),rcond=None)[0]\nplt.plot(c)\nplt.show() \n\n\n\n\n\n\n\n\nLooking at our coefficients above and how large and widely varying they are is a result of the problem being poorly conditioned, not only do these coefficients vary widely with small changes in input (try re-running the program again with noise added), but also the coefficients aren’t very intuitive at all.\n\ndef square(c):\n  return c.reshape([int(np.sqrt(len(c))), -1])\n\ndef performCorrection2D(corrupted_r, m, c):\n  return np.polynomial.polynomial.polyval2d(corrupted_r.ravel(), m.ravel(), square(c)).reshape(corrupted_r.shape)"
  },
  {
    "objectID": "posts/2021-01-29-vandermonde matrices in beam hardening corrections.html#lets-visualize-the-coefficients-to-see-what-basis-images-contribute-the-most",
    "href": "posts/2021-01-29-vandermonde matrices in beam hardening corrections.html#lets-visualize-the-coefficients-to-see-what-basis-images-contribute-the-most",
    "title": "Application of Vandermonde Interpolation in CT beam hardening correction",
    "section": "",
    "text": "Recall the correction is \\(\\hat{p} = c_{i,j} q^i M^j\\) thus the coordinates in the image below indicate the monomial and its relative contribution to the polynomial\n\ndef correctAndDisplay(corrupted_r, m, c, theta):\n  p = performCorrection2D(corrupted_r, m, c)\n  corrected = iradon(p, theta=theta, circle=True)\n  plt.imshow(corrected, cmap=plt.cm.Greys_r)\n  ms_error = mse(corrected,ideal)\n  plt.title(\"Corrected \\n MSE = \" + str(ms_error))\n  return ms_error\n\ncorrectAndDisplay(corrupted_r, m, c, theta)\n\n0.00014014461661890804\n\n\n\n\n\n\n\n\n\n\ndef visualizeCoefficients(c):\n  plt.imshow(square(c))\n  plt.xlabel('M')\n  plt.ylabel('q')\n  plt.colorbar()\n  plt.title('Monomial contributions')\n\nvisualizeCoefficients(c)\n\n\n\n\n\n\n\n\n\np = performCorrection2D(corrupted_r, m, c)\ncorrected = iradon(p, theta=theta, circle=True)\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 12))\n\nax1.set_title(\"Original\")\nax1.imshow(ideal, cmap=plt.cm.Greys_r)\n\nax2.set_title(\"Corrupted \\n MSE = \" + str(mse(uncorrected,ideal)))\nax2.imshow(uncorrected, cmap=plt.cm.Greys_r)\n\nax3.set_title(\"Corrected \\n MSE = \" + str(mse(corrected,ideal)))\nax3.imshow(corrected, cmap=plt.cm.Greys_r)\n          \nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNot bad results for 10% beam hardening!"
  },
  {
    "objectID": "posts/2021-01-29-vandermonde matrices in beam hardening corrections.html#finally-is-there-a-pattern-that-emerges-in-the-monomials-that-are-activated",
    "href": "posts/2021-01-29-vandermonde matrices in beam hardening corrections.html#finally-is-there-a-pattern-that-emerges-in-the-monomials-that-are-activated",
    "title": "Application of Vandermonde Interpolation in CT beam hardening correction",
    "section": "",
    "text": "def find2Dpolynomialcoefficients(corrupted_r, m, theta, deg):\n  f = vanderComborecon(corrupted_r,m,theta, deg)\n  return np.linalg.lstsq(f,ideal.ravel(),rcond=None)[0]\n\ndef plotAndVisualizeDegreeDependence(corrupted_r, m, theta, maxDegree=10):\n  ms_error = []\n  for d in np.arange(2, maxDegree):\n    coefficients = find2Dpolynomialcoefficients(corrupted_r, m, theta, d)\n\n    plt.figure(figsize=[14,4])\n    plt.subplot(1,3,1)\n    visualizeCoefficients(coefficients)\n    plt.subplot(1,3,2)\n    err = correctAndDisplay(corrupted_r, m, coefficients, theta)\n    ms_error.append(err)\n    plt.subplot(1,3,3)\n    plt.plot(np.arange(2, d+1), ms_error)\n    plt.xlabel('Polynomial degree')\n    plt.ylabel('MSE')\n    plt.show()\n\nplotAndVisualizeDegreeDependence(corrupted_r, m, theta)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s clear that MSE decreases and asymptotes as the polynomial degrees increase (but at the cost of"
  },
  {
    "objectID": "posts/2021-01-29-vandermonde matrices in beam hardening corrections.html#how-do-spatial-dependence-strength-vs-beam-hardening-strength-control-monomial-activation-map",
    "href": "posts/2021-01-29-vandermonde matrices in beam hardening corrections.html#how-do-spatial-dependence-strength-vs-beam-hardening-strength-control-monomial-activation-map",
    "title": "Application of Vandermonde Interpolation in CT beam hardening correction",
    "section": "",
    "text": "Try this again when there is no spatial component to the beam hardening, how does this affect the M dependency?\n\nnoSpatialm, noSpatial_corrupted_r = corruptProjection(r, bh_strength=0.1, spatialDependence_strength = 0)\nplotAndVisualizeDegreeDependence(noSpatial_corrupted_r, noSpatialm, theta)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally repeat once more with Beam hardening strength set to zero and spatiall varying strength on max, how does this affect the coefficient activation map?\n\nnoBHm, noBH_corrupted_r = corruptProjection(r, bh_strength=0, spatialDependence_strength = 1)\nplotAndVisualizeDegreeDependence(noBH_corrupted_r, noBHm, theta)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObserve how the MSE decreases then increases at higher degrees. This demonstrates a weakness of Vandermonde polynomial interpolation that as the Vandermonde matrix becomes larger its condition number grows dramatically and the result becomes unstable.\nMove on to the last notebook on iterative methods for solving the Vandermonde interpolation least squares problem: https://colab.research.google.com/drive/1kErNKAbJlJUpq2jsb0roMJA7kTNB9vY0"
  },
  {
    "objectID": "posts/fda_reasearch.html",
    "href": "posts/fda_reasearch.html",
    "title": "Research at the FDA",
    "section": "",
    "text": "When I talk to colleagues at different science, healthcare, or research institutions I realize there remain many misconceptions about what the FDA does, that we do research, and that researching at FDA you can still publish, go to conferences, apply for grants and have many of the benefits of academia while being involved in important regulatory work and influencing the development of new safe and effective medical devices. It also offers unique insights into the medical device world by sitting at the intersection of industry, academia, and the clinic.\nThe goal of this post is to share my experiences so far and why I chose to join and have stayed.\nAt the time of this posting, my research division is nestled in the following hierarchy.\n\nDivision of Imaging, Diagnostics, and Software Reliability (DIDSR)\nOffice of Science and Engineering Labs (OSEL)\nCenter for Devices and Radiological Health (CDRH)\nUS Food and Drug Administration (FDA)"
  },
  {
    "objectID": "posts/fda_reasearch.html#why-research-at-the-fda",
    "href": "posts/fda_reasearch.html#why-research-at-the-fda",
    "title": "Research at the FDA",
    "section": "",
    "text": "When I talk to colleagues at different science, healthcare, or research institutions I realize there remain many misconceptions about what the FDA does, that we do research, and that researching at FDA you can still publish, go to conferences, apply for grants and have many of the benefits of academia while being involved in important regulatory work and influencing the development of new safe and effective medical devices. It also offers unique insights into the medical device world by sitting at the intersection of industry, academia, and the clinic.\nThe goal of this post is to share my experiences so far and why I chose to join and have stayed.\nAt the time of this posting, my research division is nestled in the following hierarchy.\n\nDivision of Imaging, Diagnostics, and Software Reliability (DIDSR)\nOffice of Science and Engineering Labs (OSEL)\nCenter for Devices and Radiological Health (CDRH)\nUS Food and Drug Administration (FDA)"
  },
  {
    "objectID": "posts/2021-01-27-vandermonde interpolation.html",
    "href": "posts/2021-01-27-vandermonde interpolation.html",
    "title": "Vandermonde Interpolation Tutorial",
    "section": "",
    "text": "this tutorial is also available as colab notebook\n\nBefore diving into images let’s talk about interpolation which is at the heart of our image processing method…\n\n\nInterpolation is the act of fitting a function on some measured data points \\((x_1, y_1)\\), \\((x_2, y_2)\\), … for the purpose of infering other points.\nThis is useful say if you want to characterize some data but don’t have the time or resources to sample every possible point. With interpolation you can measure a few points then infer the rest using your fit function.\nLet’s demonstrate with an example below where we use a polynomial of degree \\(n\\) (\\(p(x)=\\sum_{i=0}^{n}c_ix^i\\)) to fit data from some arbitrary function (in practice we don’t know the function and the data is noisy).\n\nimport numpy as np \nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nnumSamples=15 #number of sampled data points\nsig=0.01\ntimeInPercent=np.linspace(0,100,numSamples)+np.random.normal(size=numSamples)*sig\nb = 10*np.exp(np.sin(0.04*timeInPercent))+np.random.normal(size=numSamples)*sig\nplt.plot(timeInPercent,b,'*')\nplt.xlabel('x % day completed [0 = 6 am, 100 = 6 pm]')\nplt.ylabel('temperature C')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe goal of Vandermonde interpolation is to transform some arbitrary input into output that lies upon the same function curve as measured by your experimental data. This agrees with our original problem statement of only having results from a few select input values.\nUsing the above situation as an example, we took 15 measurements of temperature over a day (every 48 minutes). Then the next day someone comes to us and asks what the temperature was at exactly 3:15pm yesterday [77.08 % of the day], however we don’t have a measurement for that exact time. What we want is a function that we can input an arbitrary time during the day (e.g 77.08%) and get a good estimate of the temperature at that time. We use Vandermonde interpolation to find this time to temperature transformation (function).\nPolynomials are a common choice of interpolation function because they are linear, differentiable, and straightforward to understand. We can write our polynomial to predict temperature \\(T\\) for any given time of day \\(x\\) as \\(\\hat{T}=\\sum_{n=0}^{N-1}c_n x^n\\) where we must first find the set of \\(N\\) coefficients for our \\(N-1\\) degree polynomial.\nDue to the linear nature of polynomials this polynomial function can be described as a linear transformation of vector \\(x\\) by matrix \\(A\\),\n\\[\n\\hat{T}=Ac=\n\\begin{bmatrix}\n1 & x_0 & x_0^2  & \\dots  & x_0^n \\\\\n1 & x_1 & x_1^2  & \\dots  & x_1^n \\\\\n\\vdots  &\\vdots  & \\vdots & \\ddots &\\vdots \\\\\n1 & x_m & x_m^2  & \\dots  & x_m^n         \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nc_0 \\\\\nc_1 \\\\\n\\vdots \\\\\nc_{n}\n\\end{bmatrix}\n\\]\nwhere \\(A\\) is the transposed Vandermonde matrix with each element described by \\(A_{ij} = x_i^j\\). This matrix has a size of \\(M\\) rows for each time measurement (\\(M=15\\) in our example) by \\(N+1\\) columns for the monomial terms of the degree \\(N\\) polynomial that we want to approximate our function as.\nThus by solving this linear system we can find the \\(N+1\\) \\(c\\) coefficients to best estimate temperature \\(T\\) given time \\(x\\) using the transformation \\(A\\).\nAs a concrete example of the Vandermonde matrix we can call Numpy’s vandermonde function for 3 monomials and sampling 10 even sampled points 1 through 15\n\nn=3\nx = np.arange(1,16)\nvanderMat = np.vander(x,3, increasing=True)\nprint(vanderMat)\n\n[[  1   1   1]\n [  1   2   4]\n [  1   3   9]\n [  1   4  16]\n [  1   5  25]\n [  1   6  36]\n [  1   7  49]\n [  1   8  64]\n [  1   9  81]\n [  1  10 100]\n [  1  11 121]\n [  1  12 144]\n [  1  13 169]\n [  1  14 196]\n [  1  15 225]]\n\n\n\n\n\nA polynomial fit can be described in matrix terms using the Vandermonde matrix. Say wish to characterize some unknown function \\(f(x)\\), (e.g. the -log attenuation of x-rays \\(y\\) as a function of penetration depth \\(x\\) for the purpose of eventually correcting it). We measure \\(y\\) at \\(n\\) different distances \\(x\\). This can be approximated by the following polynomial\n\\(p(x) = \\bf{A}c = \\begin{bmatrix}\n1 & x_0 & x_0^2  & \\dots  & x_0^n \\\\\n1 & x_1 & x_1^2  & \\dots  & x_1^n \\\\\n\\vdots  &\\vdots  & \\vdots & \\ddots &\\vdots \\\\\n1 & x_m & x_m^2  & \\dots  & x_m^n         \\\\\n\\end{bmatrix} \\begin{bmatrix}\nc_0\\\\\nc_1\\\\\n\\vdots\\\\\nc_n\\\\\n\\end{bmatrix} =\\begin{bmatrix}\ny_0\\\\\ny_1\\\\\n\\vdots\\\\\ny_n\\\\\n\\end{bmatrix} \\approx f(x)\\)\nfor right set of coefficients \\(c\\), the goal of polynomial fitting is to solve for those \\(n\\) coefficients\n\nn=3 #the polynomial degree \nA=np.vander(timeInPercent,n)\nx=np.linalg.lstsq(A,b,rcond=None)[0]\n\n\ndef compareEstimatewithExpected(independentVar, estimate, expected):\n  fig1 = plt.figure()\n  plt.scatter(independentVar,expected)\n  line1, = plt.plot(independentVar,y,c='orange')\n  title1=plt.title(\"1D Vandermonde polynomial fitting\")\n  plt.legend([\"data\",\"fit\"])\n  plt.show()\n  return fig1, line1, title1\n\n\ny=A@x\nfig1, line1, title1 = compareEstimatewithExpected(timeInPercent, y, b)\n\n\n\n\n\n\n\n\nHmmm… That’s close, but you can only get so far with \\(n=3\\) monomials aka a quadratic fit, \\(p(x)=c_0x^2+c_1x+c_2\\).\nWhat if we add more monomial terms?\n\nfrom matplotlib import animation, rc\nimport time\nrc('animation', html='jshtml')\n\ndef animate1(i):\n  A=np.vander(timeInPercent,i)\n  x=np.linalg.lstsq(A,b,rcond=None)[0]\n  line1.set_ydata(A@x)\n  title1.set_text(\"Degree \" + str(i) + \" polynomial fit\")\n  return line1,\n\nanimation.FuncAnimation(fig1, animate1, np.arange(0, 10), interval=500)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\nI should mention that numpy includes a convencience function for A = np.vander(time, deg, increasing=True) followed by yhat = Ax, called polyval where yhat = np.polynomial.polynomial.polyval(time, x)\nhttps://docs.scipy.org/doc/numpy-1.12.0/reference/generated/numpy.polynomial.polynomial.polyval.html#numpy.polynomial.polynomial.polyval\n\nn=6 #the polynomial degree \nA=np.vander(timeInPercent,n, increasing=True)\nx=np.linalg.lstsq(A,b,rcond=None)[0]\ny = np.polynomial.polynomial.polyval(timeInPercent, x)\n\ncompareEstimatewithExpected(timeInPercent, y, b)\n\n\n\n\n\n\n\n\n(&lt;Figure size 432x288 with 1 Axes&gt;,\n &lt;matplotlib.lines.Line2D at 0x7f52afc12470&gt;,\n Text(0.5, 1.0, '1D Vandermonde polynomial fitting'))\n\n\n\n\n\nVandermonde interpolation can be used for more than approximating an unknown function by a polynomial, it can also be used to linearize nonlinear data. This is useful in CT reconstruction which assumes a linear relationship between -log(Transmission) data and path length. The reason that this is not the case in practice is primarily because x-rays from standard tube sources are polychromatic with lower energy rays being attenuated at a greater rate than higher energy ones.\nTo correct for this we can use vandermonde interpolation to remap our measured polychromatic attenuation values (-log(Transmission)) to what we would expect had they come from monochromatic x-rays with the desired linear relationship.\nWe can demonstrate this below with a 1D example:\n\nn_samples = 10 #number of distances along x to measure attenuation through our homogenous sample of material mu\nd = np.linspace(0,1,n_samples) #vector of sampled distances\nmu = 0.2059 #attenuation coefficient of water at 60keV for this example units [cm^-2]\nlin_atten = mu*d #the desired linear attenuation of 60 keV x-rays as a function of distance d\nfig = plt.figure()\nplt.scatter(d,lin_atten,c='orange')\n#as a result of beam hardening, the relationship is no longer linear (arbitrary choice of coefficient and exponent)\nmeasured_atten = 0.4*lin_atten**0.8\n#measured_atten = np.sin(lin_atten**0.8) #uncomment to experiment with even more nonlinear functions\nline, = plt.plot(d,measured_atten,'r')\nplt.xlabel(\"distance [cm]\")\nplt.ylabel(\"Attenuation -log(Transmission)\")\ntitle=plt.title(\"-log attenuation vs distance\")\nplt.legend((\"monochromatic attenuation\",\"polychromatic attenuation\"))\nplt.show()\n\n\n\n\n\n\n\n\nWe can use the same tools developed in Vandermonde polynomial interpolation to linearize our measured polychromatic attenuation values to match what we would expect from a monochromatic beam\n\nfrom matplotlib import animation, rc\nimport time\nrc('animation', html='jshtml')\n\ndef animate(i):\n  if i == 0:\n    line.set_ydata(measured_atten)\n    title.set_text(\"original\")\n  else:\n    A = np.vander(measured_atten,i)\n    x=np.linalg.lstsq(A,lin_atten,rcond=None)[0]\n    line.set_ydata(A@x)\n    title.set_text(\"Degree \" + str(i) + \" polynomial fit\")\n  return line,\n\nanimation.FuncAnimation(fig, animate, np.arange(0, 10), interval=500)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\ndeg=5\nA = np.vander(measured_atten,deg)\nx=np.linalg.lstsq(A,lin_atten,rcond=None)[0]\ncorrected_measured=A@x\n\nplt.plot(lin_atten,measured_atten,lin_atten,corrected_measured)\nplt.xlabel(\"desired monochromatic response\")\nplt.ylabel(\"measured polychromatic respose\")\nplt.legend((\"uncorrected\",\"corrected\"))\nplt.show()\n\n\n\n\n\n\n\n\nNow go back and experiment with measured_atten giving it different nonlinear functions of d and see what order polynomial is required for the fit\nMove on to the next notebook on LU decomposition: https://colab.research.google.com/drive/1SJtGSP_1XcjLh0pyF_RwmakUMdDuNcxt"
  },
  {
    "objectID": "posts/2021-01-27-vandermonde interpolation.html#interpolation",
    "href": "posts/2021-01-27-vandermonde interpolation.html#interpolation",
    "title": "Vandermonde Interpolation Tutorial",
    "section": "",
    "text": "Interpolation is the act of fitting a function on some measured data points \\((x_1, y_1)\\), \\((x_2, y_2)\\), … for the purpose of infering other points.\nThis is useful say if you want to characterize some data but don’t have the time or resources to sample every possible point. With interpolation you can measure a few points then infer the rest using your fit function.\nLet’s demonstrate with an example below where we use a polynomial of degree \\(n\\) (\\(p(x)=\\sum_{i=0}^{n}c_ix^i\\)) to fit data from some arbitrary function (in practice we don’t know the function and the data is noisy).\n\nimport numpy as np \nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nnumSamples=15 #number of sampled data points\nsig=0.01\ntimeInPercent=np.linspace(0,100,numSamples)+np.random.normal(size=numSamples)*sig\nb = 10*np.exp(np.sin(0.04*timeInPercent))+np.random.normal(size=numSamples)*sig\nplt.plot(timeInPercent,b,'*')\nplt.xlabel('x % day completed [0 = 6 am, 100 = 6 pm]')\nplt.ylabel('temperature C')\nplt.show()"
  },
  {
    "objectID": "posts/2021-01-27-vandermonde interpolation.html#an-applied-example",
    "href": "posts/2021-01-27-vandermonde interpolation.html#an-applied-example",
    "title": "Vandermonde Interpolation Tutorial",
    "section": "",
    "text": "The goal of Vandermonde interpolation is to transform some arbitrary input into output that lies upon the same function curve as measured by your experimental data. This agrees with our original problem statement of only having results from a few select input values.\nUsing the above situation as an example, we took 15 measurements of temperature over a day (every 48 minutes). Then the next day someone comes to us and asks what the temperature was at exactly 3:15pm yesterday [77.08 % of the day], however we don’t have a measurement for that exact time. What we want is a function that we can input an arbitrary time during the day (e.g 77.08%) and get a good estimate of the temperature at that time. We use Vandermonde interpolation to find this time to temperature transformation (function).\nPolynomials are a common choice of interpolation function because they are linear, differentiable, and straightforward to understand. We can write our polynomial to predict temperature \\(T\\) for any given time of day \\(x\\) as \\(\\hat{T}=\\sum_{n=0}^{N-1}c_n x^n\\) where we must first find the set of \\(N\\) coefficients for our \\(N-1\\) degree polynomial.\nDue to the linear nature of polynomials this polynomial function can be described as a linear transformation of vector \\(x\\) by matrix \\(A\\),\n\\[\n\\hat{T}=Ac=\n\\begin{bmatrix}\n1 & x_0 & x_0^2  & \\dots  & x_0^n \\\\\n1 & x_1 & x_1^2  & \\dots  & x_1^n \\\\\n\\vdots  &\\vdots  & \\vdots & \\ddots &\\vdots \\\\\n1 & x_m & x_m^2  & \\dots  & x_m^n         \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nc_0 \\\\\nc_1 \\\\\n\\vdots \\\\\nc_{n}\n\\end{bmatrix}\n\\]\nwhere \\(A\\) is the transposed Vandermonde matrix with each element described by \\(A_{ij} = x_i^j\\). This matrix has a size of \\(M\\) rows for each time measurement (\\(M=15\\) in our example) by \\(N+1\\) columns for the monomial terms of the degree \\(N\\) polynomial that we want to approximate our function as.\nThus by solving this linear system we can find the \\(N+1\\) \\(c\\) coefficients to best estimate temperature \\(T\\) given time \\(x\\) using the transformation \\(A\\).\nAs a concrete example of the Vandermonde matrix we can call Numpy’s vandermonde function for 3 monomials and sampling 10 even sampled points 1 through 15\n\nn=3\nx = np.arange(1,16)\nvanderMat = np.vander(x,3, increasing=True)\nprint(vanderMat)\n\n[[  1   1   1]\n [  1   2   4]\n [  1   3   9]\n [  1   4  16]\n [  1   5  25]\n [  1   6  36]\n [  1   7  49]\n [  1   8  64]\n [  1   9  81]\n [  1  10 100]\n [  1  11 121]\n [  1  12 144]\n [  1  13 169]\n [  1  14 196]\n [  1  15 225]]"
  },
  {
    "objectID": "posts/2021-01-27-vandermonde interpolation.html#vandermonde-interpolation",
    "href": "posts/2021-01-27-vandermonde interpolation.html#vandermonde-interpolation",
    "title": "Vandermonde Interpolation Tutorial",
    "section": "",
    "text": "A polynomial fit can be described in matrix terms using the Vandermonde matrix. Say wish to characterize some unknown function \\(f(x)\\), (e.g. the -log attenuation of x-rays \\(y\\) as a function of penetration depth \\(x\\) for the purpose of eventually correcting it). We measure \\(y\\) at \\(n\\) different distances \\(x\\). This can be approximated by the following polynomial\n\\(p(x) = \\bf{A}c = \\begin{bmatrix}\n1 & x_0 & x_0^2  & \\dots  & x_0^n \\\\\n1 & x_1 & x_1^2  & \\dots  & x_1^n \\\\\n\\vdots  &\\vdots  & \\vdots & \\ddots &\\vdots \\\\\n1 & x_m & x_m^2  & \\dots  & x_m^n         \\\\\n\\end{bmatrix} \\begin{bmatrix}\nc_0\\\\\nc_1\\\\\n\\vdots\\\\\nc_n\\\\\n\\end{bmatrix} =\\begin{bmatrix}\ny_0\\\\\ny_1\\\\\n\\vdots\\\\\ny_n\\\\\n\\end{bmatrix} \\approx f(x)\\)\nfor right set of coefficients \\(c\\), the goal of polynomial fitting is to solve for those \\(n\\) coefficients\n\nn=3 #the polynomial degree \nA=np.vander(timeInPercent,n)\nx=np.linalg.lstsq(A,b,rcond=None)[0]\n\n\ndef compareEstimatewithExpected(independentVar, estimate, expected):\n  fig1 = plt.figure()\n  plt.scatter(independentVar,expected)\n  line1, = plt.plot(independentVar,y,c='orange')\n  title1=plt.title(\"1D Vandermonde polynomial fitting\")\n  plt.legend([\"data\",\"fit\"])\n  plt.show()\n  return fig1, line1, title1\n\n\ny=A@x\nfig1, line1, title1 = compareEstimatewithExpected(timeInPercent, y, b)\n\n\n\n\n\n\n\n\nHmmm… That’s close, but you can only get so far with \\(n=3\\) monomials aka a quadratic fit, \\(p(x)=c_0x^2+c_1x+c_2\\).\nWhat if we add more monomial terms?\n\nfrom matplotlib import animation, rc\nimport time\nrc('animation', html='jshtml')\n\ndef animate1(i):\n  A=np.vander(timeInPercent,i)\n  x=np.linalg.lstsq(A,b,rcond=None)[0]\n  line1.set_ydata(A@x)\n  title1.set_text(\"Degree \" + str(i) + \" polynomial fit\")\n  return line1,\n\nanimation.FuncAnimation(fig1, animate1, np.arange(0, 10), interval=500)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/2021-01-27-vandermonde interpolation.html#an-abbreviated-version",
    "href": "posts/2021-01-27-vandermonde interpolation.html#an-abbreviated-version",
    "title": "Vandermonde Interpolation Tutorial",
    "section": "",
    "text": "I should mention that numpy includes a convencience function for A = np.vander(time, deg, increasing=True) followed by yhat = Ax, called polyval where yhat = np.polynomial.polynomial.polyval(time, x)\nhttps://docs.scipy.org/doc/numpy-1.12.0/reference/generated/numpy.polynomial.polynomial.polyval.html#numpy.polynomial.polynomial.polyval\n\nn=6 #the polynomial degree \nA=np.vander(timeInPercent,n, increasing=True)\nx=np.linalg.lstsq(A,b,rcond=None)[0]\ny = np.polynomial.polynomial.polyval(timeInPercent, x)\n\ncompareEstimatewithExpected(timeInPercent, y, b)\n\n\n\n\n\n\n\n\n(&lt;Figure size 432x288 with 1 Axes&gt;,\n &lt;matplotlib.lines.Line2D at 0x7f52afc12470&gt;,\n Text(0.5, 1.0, '1D Vandermonde polynomial fitting'))"
  },
  {
    "objectID": "posts/2021-01-27-vandermonde interpolation.html#vondermonde-interpolation-for-linearization",
    "href": "posts/2021-01-27-vandermonde interpolation.html#vondermonde-interpolation-for-linearization",
    "title": "Vandermonde Interpolation Tutorial",
    "section": "",
    "text": "Vandermonde interpolation can be used for more than approximating an unknown function by a polynomial, it can also be used to linearize nonlinear data. This is useful in CT reconstruction which assumes a linear relationship between -log(Transmission) data and path length. The reason that this is not the case in practice is primarily because x-rays from standard tube sources are polychromatic with lower energy rays being attenuated at a greater rate than higher energy ones.\nTo correct for this we can use vandermonde interpolation to remap our measured polychromatic attenuation values (-log(Transmission)) to what we would expect had they come from monochromatic x-rays with the desired linear relationship.\nWe can demonstrate this below with a 1D example:\n\nn_samples = 10 #number of distances along x to measure attenuation through our homogenous sample of material mu\nd = np.linspace(0,1,n_samples) #vector of sampled distances\nmu = 0.2059 #attenuation coefficient of water at 60keV for this example units [cm^-2]\nlin_atten = mu*d #the desired linear attenuation of 60 keV x-rays as a function of distance d\nfig = plt.figure()\nplt.scatter(d,lin_atten,c='orange')\n#as a result of beam hardening, the relationship is no longer linear (arbitrary choice of coefficient and exponent)\nmeasured_atten = 0.4*lin_atten**0.8\n#measured_atten = np.sin(lin_atten**0.8) #uncomment to experiment with even more nonlinear functions\nline, = plt.plot(d,measured_atten,'r')\nplt.xlabel(\"distance [cm]\")\nplt.ylabel(\"Attenuation -log(Transmission)\")\ntitle=plt.title(\"-log attenuation vs distance\")\nplt.legend((\"monochromatic attenuation\",\"polychromatic attenuation\"))\nplt.show()\n\n\n\n\n\n\n\n\nWe can use the same tools developed in Vandermonde polynomial interpolation to linearize our measured polychromatic attenuation values to match what we would expect from a monochromatic beam\n\nfrom matplotlib import animation, rc\nimport time\nrc('animation', html='jshtml')\n\ndef animate(i):\n  if i == 0:\n    line.set_ydata(measured_atten)\n    title.set_text(\"original\")\n  else:\n    A = np.vander(measured_atten,i)\n    x=np.linalg.lstsq(A,lin_atten,rcond=None)[0]\n    line.set_ydata(A@x)\n    title.set_text(\"Degree \" + str(i) + \" polynomial fit\")\n  return line,\n\nanimation.FuncAnimation(fig, animate, np.arange(0, 10), interval=500)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\ndeg=5\nA = np.vander(measured_atten,deg)\nx=np.linalg.lstsq(A,lin_atten,rcond=None)[0]\ncorrected_measured=A@x\n\nplt.plot(lin_atten,measured_atten,lin_atten,corrected_measured)\nplt.xlabel(\"desired monochromatic response\")\nplt.ylabel(\"measured polychromatic respose\")\nplt.legend((\"uncorrected\",\"corrected\"))\nplt.show()\n\n\n\n\n\n\n\n\nNow go back and experiment with measured_atten giving it different nonlinear functions of d and see what order polynomial is required for the fit\nMove on to the next notebook on LU decomposition: https://colab.research.google.com/drive/1SJtGSP_1XcjLh0pyF_RwmakUMdDuNcxt"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "SPIE Medical Imaging 2019\n\n\nConference Proceedings Paper pre-print# 2019 ## Visibility Guided Phase Contrast Denoising\nSPIE Medical Imaging 2019\n\n\nConference Proceedings Paper pre-print"
  },
  {
    "objectID": "presentations.html#visibility-guided-phase-contrast-denoising",
    "href": "presentations.html#visibility-guided-phase-contrast-denoising",
    "title": "Presentations",
    "section": "",
    "text": "SPIE Medical Imaging 2019\n\n\nConference Proceedings Paper pre-print# 2019 ## Visibility Guided Phase Contrast Denoising\nSPIE Medical Imaging 2019\n\n\nConference Proceedings Paper pre-print"
  },
  {
    "objectID": "posts/quarto.html",
    "href": "posts/quarto.html",
    "title": "Migrating to Quarto",
    "section": "",
    "text": "This site has moved to Quarto from fastpages, originally recommended for blogging by fast.ai, here’s a discussion on why"
  },
  {
    "objectID": "posts/2021-01-31-Empirical Beam Hardening Correction for X-Ray Grating Interferometry.html",
    "href": "posts/2021-01-31-Empirical Beam Hardening Correction for X-Ray Grating Interferometry.html",
    "title": "Empirical Beam Hardening Correction for X-Ray Grating Interferometry (EBHC-GI)",
    "section": "",
    "text": "Recently, my first first-author manuscript related to my thesis work on x-ray phase contrast was published. The paper summarizes artifacts experienced while using a tabletop x-ray phase contrast micro-CT system. While the focus of the paper is limited to grating-based x-ray phase contrast systems, the empirical methods used to solve the beam hardening problem are quite generalizable. I first learned about these interpolation-based empirical corrections from researching previous x-ray corrections that are referenced in the paper, but along the way I enjoyed digging more into the math of solving least squares problems and how they are implemented. The following tutorials document some of my explorations and provide background to the paper."
  },
  {
    "objectID": "posts/2021-01-31-Empirical Beam Hardening Correction for X-Ray Grating Interferometry.html#manuscript-citation-and-links",
    "href": "posts/2021-01-31-Empirical Beam Hardening Correction for X-Ray Grating Interferometry.html#manuscript-citation-and-links",
    "title": "Empirical Beam Hardening Correction for X-Ray Grating Interferometry (EBHC-GI)",
    "section": "Manuscript Citation and Links",
    "text": "Manuscript Citation and Links\n\nNelson, B.J., Leng, S., Shanblatt, E.R., McCollough, C.H. and Koenig, T. (2021), Empirical beam hardening and ring artifact correction for x‐ray grating interferometry (EBHC‐GI). Med Phys. https://doi.org/10.1002/mp.14672\n\npre-peer reviewed version"
  },
  {
    "objectID": "posts/2021-01-31-Empirical Beam Hardening Correction for X-Ray Grating Interferometry.html#related-tutorials",
    "href": "posts/2021-01-31-Empirical Beam Hardening Correction for X-Ray Grating Interferometry.html#related-tutorials",
    "title": "Empirical Beam Hardening Correction for X-Ray Grating Interferometry (EBHC-GI)",
    "section": "Related Tutorials",
    "text": "Related Tutorials\n\nsolving interpolation problems with Vandermonde Matrices\nperforming matrix inversion via LU decomposition\nusing interpolation to fix image artifacts in CT\n\nthis tutorial is built upon the previous two but is most related to the paper"
  },
  {
    "objectID": "posts/2021-01-28-lu solving.html",
    "href": "posts/2021-01-28-lu solving.html",
    "title": "Direct Inversion via LU Decomposition",
    "section": "",
    "text": "this tutorial is also available as a colab notebook\n\nContinuing with the Vandermonde interpolation example this tutorial goes step by step on how computers typically implement matrix inversions for the purpose of solving least squares problems.\nThis typically takes the form of a matrix factorization of the matrix to be inverted followed by forward and back substitution. The matrix factorization demoed here is LU decomposition. We compare our hand-coded solvers with Numpy library solvers to compare results.\n\ncd 'drive/My Drive/Colab Notebooks/Solving Least Squares Problems'\n\n[Errno 2] No such file or directory: 'drive/My Drive/Colab Notebooks/Solving Least Squares Problems'\n/content"
  },
  {
    "objectID": "posts/2021-01-28-lu solving.html#introduction-to-lu-decomposition",
    "href": "posts/2021-01-28-lu solving.html#introduction-to-lu-decomposition",
    "title": "Direct Inversion via LU Decomposition",
    "section": "Introduction to LU decomposition",
    "text": "Introduction to LU decomposition\nOur goal is to explore how to solve systems of linear equations following Trefethan and fastai’s numerical linear algebrea course: https://www.youtube.com/watch?v=O2x5KPJr5ag&list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY&index=5."
  },
  {
    "objectID": "posts/2020-09-08-juliacon2020.html",
    "href": "posts/2020-09-08-juliacon2020.html",
    "title": "JuliaCon 2020",
    "section": "",
    "text": "mug shot\n\n\nJulia is an exciting new language (1.0 released in 2018) well suited for scientific computing. More impressive is the vibrant Julia community of users and developers. JuliaCon is an annual meetup that showcases the exciting work going on around the Julia ecosystem. This was my first year attending (albeit virtually!) and I am still working to get caught up on all of the great talks! In this post I’ll curate some of my favorite talks from this year and their relevance to my work."
  },
  {
    "objectID": "posts/2020-09-08-juliacon2020.html#juliacon-2020",
    "href": "posts/2020-09-08-juliacon2020.html#juliacon-2020",
    "title": "JuliaCon 2020",
    "section": "",
    "text": "mug shot\n\n\nJulia is an exciting new language (1.0 released in 2018) well suited for scientific computing. More impressive is the vibrant Julia community of users and developers. JuliaCon is an annual meetup that showcases the exciting work going on around the Julia ecosystem. This was my first year attending (albeit virtually!) and I am still working to get caught up on all of the great talks! In this post I’ll curate some of my favorite talks from this year and their relevance to my work."
  },
  {
    "objectID": "posts/2020-09-08-juliacon2020.html#building-microservices-and-applications-in-julia",
    "href": "posts/2020-09-08-juliacon2020.html#building-microservices-and-applications-in-julia",
    "title": "JuliaCon 2020",
    "section": "Building Microservices and Applications in Julia",
    "text": "Building Microservices and Applications in Julia\nThis is a great workshop on how to organize Julia code into modules for effective code reuse. At first I found the compiled nature of Julia to make building Julia modules and packages more complicated than doing so in Python, Jacob Quin does a nice job breaking the process down."
  },
  {
    "objectID": "posts/2020-09-08-juliacon2020.html#using-vs-code-for-julia-developement",
    "href": "posts/2020-09-08-juliacon2020.html#using-vs-code-for-julia-developement",
    "title": "JuliaCon 2020",
    "section": "Using VS Code for Julia Developement",
    "text": "Using VS Code for Julia Developement\nI use VS Code as my IDE of choice for most work and find that the Julia extension is a must when writing Julia. I learned several new tips from this talk including how to run julia scripts e.g. script.jl like Jupyter Notebooks! E.g. &lt;ctrl&gt; + &lt;enter&gt; to run a single line to the REPL (or printing the result right next to the line itself aftering changing the settings) and &lt;shift&gt; + &lt;enter&gt; to run a cell Jupyter style.\n## cell 1\na = 1\nb = 2\nc = a + b\nprintln(\"The sum of $a and $b is $c\")\n## cell 2\nprintln(\"hitting &lt;shift&gt; + &lt;enter&gt; runs the next cell\")\n##"
  },
  {
    "objectID": "posts/2020-09-08-juliacon2020.html#pluto.jl",
    "href": "posts/2020-09-08-juliacon2020.html#pluto.jl",
    "title": "JuliaCon 2020",
    "section": "Pluto.jl",
    "text": "Pluto.jl\nI find Pluto to a be a real joy to experiment in. I find it great for following along with tutorials where I can write notes and link to resources, much like a Jupyter notebook 1.\nA key difference where Pluto notebooks stand apart from Jupyter Notebooks is that it is a reactive notebook which didn’t really set in for me until I started playing around with Pluto for a while. Basically whenever I change a variable or function in one cell, all other cells referencing that variable get updated automatically. This avoids a recurring issue in Jupyter Notebooks that each cell must be run sequentially, jumping around and changing variable names or values can affect the reproducibility of the results. This is avoided in Pluto where all values are updated simultaneously. Additionally Pluto makes use of multiple threads so this all feels pretty lightweight and responsive when doing small computations.\nI have however experienced some friction while using Pluto for large array calculations common in my imagnig research work. In these instances I experience Pluto feeling lagging and sometimes crashing requiring a &lt;ctrl&gt; + c kill in the terminal. My current workflow is to experiment and take notes with toy examples in Pluto then move to writing a script in VScode for a more performant feel.\nAll in all I think many of these wrinkles will be ironed out in time. While I still use Jupyter notebooks regularly, what ultimately What keeps me coming back to Pluto is the reactive notebook and the ease of using version control such as git which makes the notebooks easier to share."
  },
  {
    "objectID": "posts/2020-09-08-juliacon2020.html#drwatson",
    "href": "posts/2020-09-08-juliacon2020.html#drwatson",
    "title": "JuliaCon 2020",
    "section": "DrWatson",
    "text": "DrWatson\nDrWatson.jl is another Julia package that I learned about from this year’s JuliaCon that I have incorportated into many of my research projects. What initially sold me were several of the defined convenience functions such as initialize_project and @quickactivate that help me better organize and share my projects (consistent with the recommendations of one of my favorite best practices papers).\n\nWhile there is much more functinality in this package these are the first two functions I started using immediately and continue to use it in my projects today. I even have using DrWatson in my ~/.julia/config/startup.jl to more easily run @quickactivate anytime I cd into my project directory and run Julia."
  },
  {
    "objectID": "posts/2020-09-08-juliacon2020.html#footnotes",
    "href": "posts/2020-09-08-juliacon2020.html#footnotes",
    "title": "JuliaCon 2020",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFun fact: Jupyter stands for Julia, Python, and R, even though many language kernels are supported.↩︎"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Nelson BJ, Kc P, Badal A, Jiang L, Masters SC, Zeng R. Pediatric evaluations for deep learning CT denoising. Medical Physics. 2024;51(2):978-990. doi:10.1002/mp.16901\nNelson BJ, Gomez-Cardona DG, Thorne JE, et al. Multiple Kernel Synthesis of Head CT Using a Task-Based Loss Function. J Digit Imaging Inform med. Published online January 12, 2024. doi:10.1007/s10278-023-00959-x\nNelson BJ, Zeng R, Sammer M, Frush DP, Delfino JG. An FDA Guide on Indications for Use and Device Reporting of Artificial Intelligence-Enabled Devices: Significance for Pediatric Use. Journal of the American College of Radiology. Published online July 1, 2023. doi:10.1016/j.jacr.2023.06.004\nNelson, B.J., Leng, S., Shanblatt, E.R., McCollough, C.H. and Koenig, T. (2021), Empirical beam hardening and ring artifact correction for x‐ray grating interferometry (EBHC‐GI). Med Phys. https://doi.org/10.1002/mp.14672\n\npre-peer reviewed version\n\nSung, Y., Nelson, B., Shanblatt, E.R., Gupta, R., McCollough, C.H. and Graves, W.S. (2020), Wave optics simulation of grating‐based X‐ray phase‐contrast imaging using 4D Mouse Whole Body (MOBY) phantom. Med. Phys, 47: 5761-5771. https://doi.org/10.1002/mp.14479\n\n\n\nElisabeth R. Shanblatt, Yongjin Sung, Rajiv Gupta, Brandon J. Nelson, Shuai Leng, William S. Graves, and Cynthia H. McCollough, “Forward model for propagation-based x-ray phase contrast imaging in parallel- and cone-beam geometry,” Opt. Express 27, 4504-4521 (2019). doi:10.1364/OE.27.004504\nSung Y, Gupta R, Nelson B, Leng S, McCollough CH, Graves WS. Phase-contrast imaging with a compact x-ray light source: system design. J Med Imaging (Bellingham). 2017;4(4):043503. doi:10.1117/1.JMI.4.4.043503\nSung Y, Nelson B, Shanblatt ER, Gupta R, McCollough CH, Graves WS. Wave optics simulation of grating-based X-ray phase-contrast imaging using 4D Mouse Whole Body (MOBY) phantom. Medical Physics. 2020;47(11):5761-5771. doi:https://doi.org/10.1002/mp.14479"
  },
  {
    "objectID": "publications.html#peer-reviewed-publications",
    "href": "publications.html#peer-reviewed-publications",
    "title": "Publications",
    "section": "",
    "text": "Nelson BJ, Kc P, Badal A, Jiang L, Masters SC, Zeng R. Pediatric evaluations for deep learning CT denoising. Medical Physics. 2024;51(2):978-990. doi:10.1002/mp.16901\nNelson BJ, Gomez-Cardona DG, Thorne JE, et al. Multiple Kernel Synthesis of Head CT Using a Task-Based Loss Function. J Digit Imaging Inform med. Published online January 12, 2024. doi:10.1007/s10278-023-00959-x\nNelson BJ, Zeng R, Sammer M, Frush DP, Delfino JG. An FDA Guide on Indications for Use and Device Reporting of Artificial Intelligence-Enabled Devices: Significance for Pediatric Use. Journal of the American College of Radiology. Published online July 1, 2023. doi:10.1016/j.jacr.2023.06.004\nNelson, B.J., Leng, S., Shanblatt, E.R., McCollough, C.H. and Koenig, T. (2021), Empirical beam hardening and ring artifact correction for x‐ray grating interferometry (EBHC‐GI). Med Phys. https://doi.org/10.1002/mp.14672\n\npre-peer reviewed version\n\nSung, Y., Nelson, B., Shanblatt, E.R., Gupta, R., McCollough, C.H. and Graves, W.S. (2020), Wave optics simulation of grating‐based X‐ray phase‐contrast imaging using 4D Mouse Whole Body (MOBY) phantom. Med. Phys, 47: 5761-5771. https://doi.org/10.1002/mp.14479\n\n\n\nElisabeth R. Shanblatt, Yongjin Sung, Rajiv Gupta, Brandon J. Nelson, Shuai Leng, William S. Graves, and Cynthia H. McCollough, “Forward model for propagation-based x-ray phase contrast imaging in parallel- and cone-beam geometry,” Opt. Express 27, 4504-4521 (2019). doi:10.1364/OE.27.004504\nSung Y, Gupta R, Nelson B, Leng S, McCollough CH, Graves WS. Phase-contrast imaging with a compact x-ray light source: system design. J Med Imaging (Bellingham). 2017;4(4):043503. doi:10.1117/1.JMI.4.4.043503\nSung Y, Nelson B, Shanblatt ER, Gupta R, McCollough CH, Graves WS. Wave optics simulation of grating-based X-ray phase-contrast imaging using 4D Mouse Whole Body (MOBY) phantom. Medical Physics. 2020;47(11):5761-5771. doi:https://doi.org/10.1002/mp.14479"
  },
  {
    "objectID": "publications.html#conference-proceedings",
    "href": "publications.html#conference-proceedings",
    "title": "Publications",
    "section": "Conference Proceedings",
    "text": "Conference Proceedings\n\nComplementary use of x-ray dark-field and attenuation computed tomography in quantifying pulmonary fibrosis in a mouse model\n\nNelson BJ, Leng S, Koenig T, McCollough CH. Complementary use of x-ray dark-field and attenuation computed tomography in quantifying pulmonary fibrosis in a mouse model. In: Medical Imaging 2022: Biomedical Applications in Molecular, Structural, and Functional Imaging. Vol 12036. SPIE; 2022:253-263. doi:10.1117/12.2612877\n\n\n\nDemonstration of Phase-Assisted Material Decomposition with a Talbot-Lau Interferometer using a Single X-Ray Tube Potential\n\nElisabeth R. Shanblatt, Brandon J. Nelson, Shengzhen Tao, Shuai Leng, Cynthia H. McCollough\nSPIE Medical Imaging 2019, San Diego, CA\n\n\n\nVisibility Guided Phase Contrast Denoising\n\nBrandon J. Nelson, Thomas Koenig, Elisabeth R. Shanblatt, Shuai Leng, Cynthia H. McCollough\nSPIE Medical Imaging 2019, San Diego, CA"
  }
]